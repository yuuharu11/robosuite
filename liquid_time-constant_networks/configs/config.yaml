defaults:
  - _self_
  - experiment: default # Default experiment configuration
  # # - loader: default # Dataloader (e.g. handles batches)
  # # - dataset: cifar # Defines the data (x and y pairs)
  # # - task: multiclass_classification # Defines loss and metrics
  # # - encoder: null # Interface between data and model
  # # - decoder: null # Interface between model and targets

  - callbacks: # Extra pytorch-lightning features
    - base
    - checkpoint

# Additional arguments used to configure the training loop
# Most of these set combinations of options in the PL trainer, add callbacks, or add features to the optimizer
train:
  seed: 10
  # These three options are used by callbacks (checkpoint, monitor) and scheduler
  # Most of them are task dependent and are set by the pipeline
  interval: ??? # Should be specified by scheduler. Also used by LR monitor
  monitor: ??? # Should be specified by pipeline. Used by scheduler (plateau) and checkpointer
  mode: ??? # Should be specified by pipeline. Used by scheduler (plateau) and checkpointer
  ema: 0.0 # Moving average model for validation # TODO move into callback
  test: False # Test after training
  test_only: False # Only run the test set, do not train
  learn: True # Whether to train the model
  debug: False # Special settings to make debugging more convenient
  ignore_warnings: False # Disable python warnings

  # These control state passing between batches
  state:
    mode: null # [ None | 'none' | 'reset' | 'bptt' | 'tbptt' ]
    n_context: 0 # How many steps to use as memory context. Must be >= 0 or None (null), meaning infinite context
    n_context_eval: ${.n_context} # Context at evaluation time
  # Convenience keys to allow grouping runs

  ckpt: null # Resume training

  disable_dataset: False # Disable dataset loading
  validate_at_start: false

  pretrained_model_path: null # Path to pretrained model
  pretrained_model_strict_load: true # Whether to load the pretrained model even if the model is not compatible
  pretrained_model_state_hook: # Hook called on the loaded model's state_dict
    _name_: null
  post_init_hook: # After initializing model, call method on model
    _name_: null

  layer_decay: # Used for ImageNet finetuning
    _name_: null
    decay: 0.7

  ### add function to prevent training from catastrophic forgetting
  # replay based
  replay:
    _name_: null #[none, exact_replay, reservoir]
    memory_size: 0 # Size of the memory buffer, 0 means no memory
    batch_size: 0 # Batch size for the replay function, 0 means no replay
    n_replay: 1 # Number of times to replay the memory in each epoch
    buffer_path: /work/buffer/replay_buffer.pt # Path to the memory buffer, if not specified, it will be created in the current directory

  regularization:
    _name_: null #[none, ewc, si, mas]
    lambda: 0.0 # Regularization strength
    param_path: null # Path to the parameters for the regularization, if not specified, it will be created in the current directory
    max_ewc_datasize: 1000 # Maximum number of data points to use
    task_id: 0 # Task ID for CIL, should be set externally before training each task

  architecture:
    _name_: null # [pnn, packnet]Architecture specific modifications to the model
    pruning_rate: 0.0 # Pruning rate for PackNet, only used if architecture is packnet
    mask_path: null # Path to the masks for PackNet, if not specified, it will be created in the current directory
    

tolerance: # fault tolerance for training on preemptible machines
  logdir: ./resume
  id: null # must be set to resume training on preemption

# We primarily use wandb so this is moved to top level in the config for convenience
# Set `~wandb` or `wandb=null` or `wandb.mode=disabled` to disable logging
# If other loggers are added, it would make sense to put this one level lower under train/ or logger/
wandb:
  project: default
  group: ""
  job_type: training
  mode: online # choices=['online', 'offline', 'disabled']
  save_dir: null
  id: null # pass correct id to resume experiment!
  name: null
  # Below options should not need to be specified
  # entity: ""  # set to name of your wandb team or just remove it
  # log_model: False
  # prefix: ""
  # job_type: "train"
  tags: []

hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S-%f}
  job:
    chdir: true